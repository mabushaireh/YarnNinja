Container: container_e145_1647400556153_0012_01_000001 on wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net_30050_1647414834857
LogAggregationType: AGGREGATED
=============================================================================================================================================
LogType:directory.info
LogLastModifiedTime:Wed Mar 16 07:13:54 +0000 2022
LogLength:7271
LogContents:
ls -l:
total 40
-rw-r--r-- 1 yarn hadoop   74 Mar 16 06:37 container_tokens
-rwx------ 1 yarn hadoop  714 Mar 16 06:37 default_container_executor_session.sh
-rwx------ 1 yarn hadoop  769 Mar 16 06:37 default_container_executor.sh
lrwxrwxrwx 1 yarn hadoop  102 Mar 16 06:37 __hive_libs__ -> /mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21998/__hive_libs__6588329932098669616.zip
-rwx------ 1 yarn hadoop 6285 Mar 16 06:37 launch_container.sh
lrwxrwxrwx 1 yarn hadoop   85 Mar 16 06:37 py4j-0.10.7-src.zip -> /mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21997/py4j-0.10.7-src.zip
lrwxrwxrwx 1 yarn hadoop   77 Mar 16 06:37 pyspark.zip -> /mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21995/pyspark.zip
lrwxrwxrwx 1 yarn hadoop   84 Mar 16 06:37 __spark_conf__ -> /mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21996/__spark_conf__.zip
drwx--x--- 2 yarn hadoop 4096 Mar 16 06:37 tmp
find -L . -maxdepth 5 -ls:
 31326214      4 drwx--x---   3 yarn     hadoop       4096 Mar 16 06:37 .
 31326218      4 -rw-r--r--   1 yarn     hadoop         74 Mar 16 06:37 ./container_tokens
 31326215      4 drwx--x---   2 yarn     hadoop       4096 Mar 16 06:37 ./tmp
 25824581    580 -r-x------   1 yarn     hadoop     592826 Mar 16 06:37 ./pyspark.zip
 31326232      4 -rw-r--r--   1 yarn     hadoop         60 Mar 16 06:37 ./.launch_container.sh.crc
 31326233      4 -rwx------   1 yarn     hadoop        714 Mar 16 06:37 ./default_container_executor_session.sh
 25824584      4 drwx------   3 yarn     hadoop       4096 Mar 16 06:37 ./__spark_conf__
 25824586      8 -r-x------   1 yarn     hadoop       5142 Mar 16 06:37 ./__spark_conf__/metrics.properties
 25824585      4 -r-x------   1 yarn     hadoop       3333 Mar 16 06:37 ./__spark_conf__/log4j.properties
 25824622    156 -r-x------   1 yarn     hadoop     157911 Mar 16 06:37 ./__spark_conf__/__spark_hadoop_conf__.xml
 25824587      4 drwx------   2 yarn     hadoop       4096 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__
 25824590      4 -r-x------   1 yarn     hadoop       2591 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/yarnservice-log4j.properties
 25824594     32 -r-x------   1 yarn     hadoop      29924 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/yarn-site.xml
 25824620      4 -r-x------   1 yarn     hadoop        945 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/taskcontroller.cfg
 25824609      4 -r-x------   1 yarn     hadoop        311 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/resource-types.xml
 25824618      4 -r-x------   1 yarn     hadoop       1044 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/ssl-server.xml
 25824605      8 -r-x------   1 yarn     hadoop       5296 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/yarn-env.sh
 25824621      8 -r-x------   1 yarn     hadoop       4113 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/mapred-queues.xml.template
 25824613      4 -r-x------   1 yarn     hadoop       1344 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hive-site.xml
 25824602      4 -r-x------   1 yarn     hadoop       2846 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/capacity-scheduler.xml
 25824593      4 -r-x------   1 yarn     hadoop        241 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hadoop-metrics2-adl-file-system.properties
 25824591     12 -r-x------   1 yarn     hadoop       8436 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/mapred-site.xml
 25824589     12 -r-x------   1 yarn     hadoop      10621 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/log4j.properties
 25824592      4 -r-x------   1 yarn     hadoop       2123 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hadoop-metrics2.properties
 25824604      4 -r-x------   1 yarn     hadoop       1335 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/configuration.xsl
 25824608      4 -r-x------   1 yarn     hadoop       1020 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/commons-logging.properties
 25824612      4 -r-x------   1 yarn     hadoop       1602 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/health_check
 25824615      4 -r-x------   1 yarn     hadoop       1352 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hadoop-policy.xml
 25824596      4 -r-x------   1 yarn     hadoop          1 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/yarn.exclude
 25824600      4 -r-x------   1 yarn     hadoop          1 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/dfs.exclude
 25824616      4 -r-x------   1 yarn     hadoop         65 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/slaves
 25824599      4 -r-x------   1 yarn     hadoop        244 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/spark-thrift-fairscheduler.xml
 25824607      4 -r-x------   1 yarn     hadoop       1674 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/container-executor.cfg
 25824588      8 -r-x------   1 yarn     hadoop       6195 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hadoop-env.sh
 25824614      4 -r-x------   1 yarn     hadoop       2316 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/ssl-client.xml.example
 25824606     12 -r-x------   1 yarn     hadoop       9223 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hdfs-site.xml
 25824595      4 -r-x------   1 yarn     hadoop        259 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hadoop-metrics2-azure-file-system.properties
 25824617      4 -r-x------   1 yarn     hadoop        122 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/topology_mappings.data
 25824619      4 -r-x------   1 yarn     hadoop       2697 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/ssl-server.xml.example
 25824611      4 -r-x------   1 yarn     hadoop       1282 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/mapred-env.sh
 25824601      4 -r-x------   1 yarn     hadoop        845 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/ssl-client.xml
 25824610      8 -r-x------   1 yarn     hadoop       4221 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/task-log4j.properties
 25824603      4 -r-x------   1 yarn     hadoop       2358 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/topology_script.py
 25824597     12 -r-x------   1 yarn     hadoop      11445 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/core-site.xml
 25824598      4 -r-x------   1 yarn     hadoop         10 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/workers
 25824623      8 -r-x------   1 yarn     hadoop       5017 Mar 16 06:37 ./__spark_conf__/__spark_conf__.properties
 25824628      4 drwx------   2 yarn     hadoop       4096 Mar 16 06:37 ./__hive_libs__
 25824629  48644 -r-x------   1 yarn     hadoop   49809546 Mar 16 06:37 ./__hive_libs__/standalone-metastore-1.21.4.4.1.4.8-hive3.jar
 25824625     44 -r-x------   1 yarn     hadoop      42437 Mar 16 06:37 ./py4j-0.10.7-src.zip
 31326240      4 -rw-r--r--   1 yarn     hadoop         16 Mar 16 06:37 ./.default_container_executor.sh.crc
 31326239      4 -rwx------   1 yarn     hadoop        769 Mar 16 06:37 ./default_container_executor.sh
 31326231      8 -rwx------   1 yarn     hadoop       6285 Mar 16 06:37 ./launch_container.sh
 31326219      4 -rw-r--r--   1 yarn     hadoop         12 Mar 16 06:37 ./.container_tokens.crc
 31326234      4 -rw-r--r--   1 yarn     hadoop         16 Mar 16 06:37 ./.default_container_executor_session.sh.crc
broken symlinks(find -L . -maxdepth 5 -type l -ls):

End of LogType:directory.info
*******************************************************************************

Container: container_e145_1647400556153_0012_01_000001 on wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net_30050_1647414834857
LogAggregationType: AGGREGATED
=============================================================================================================================================
LogType:launch_container.sh
LogLastModifiedTime:Wed Mar 16 07:13:54 +0000 2022
LogLength:6285
LogContents:
#!/bin/bash

set -o pipefail -e
export PRELAUNCH_OUT="/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000001/prelaunch.out"
exec >"${PRELAUNCH_OUT}"
export PRELAUNCH_ERR="/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000001/prelaunch.err"
exec 2>"${PRELAUNCH_ERR}"
echo "Setting up env variables"
export JAVA_HOME=${JAVA_HOME:-"/usr/lib/jvm/zulu-8-azure-amd64"}
export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-"/usr/hdp/4.1.4.8/hadoop/conf"}
export HADOOP_YARN_HOME=${HADOOP_YARN_HOME:-"/usr/hdp/4.1.4.8/hadoop-yarn"}
export HADOOP_HOME=${HADOOP_HOME:-"/usr/hdp/4.1.4.8/hadoop"}
export PATH=${PATH:-"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/var/lib/ambari-agent"}
export LANG=${LANG:-"en_US.UTF-8"}
export HADOOP_TOKEN_FILE_LOCATION="/mnt/resource/hadoop/yarn/local/usercache/sshuser/appcache/application_1647400556153_0012/container_e145_1647400556153_0012_01_000001/container_tokens"
export CONTAINER_ID="container_e145_1647400556153_0012_01_000001"
export NM_PORT="30050"
export NM_HOST="wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net"
export NM_HTTP_PORT="30060"
export LOCAL_DIRS="/mnt/resource/hadoop/yarn/local/usercache/sshuser/appcache/application_1647400556153_0012"
export LOCAL_USER_DIRS="/mnt/resource/hadoop/yarn/local/usercache/sshuser/"
export LOG_DIRS="/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000001"
export USER="sshuser"
export LOGNAME="sshuser"
export HOME="/home/"
export PWD="/mnt/resource/hadoop/yarn/local/usercache/sshuser/appcache/application_1647400556153_0012/container_e145_1647400556153_0012_01_000001"
export JVM_PID="$$"
export MALLOC_ARENA_MAX="4"
export NM_AUX_SERVICE_mapreduce_shuffle="AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="
export NM_AUX_SERVICE_spark2_shuffle=""
export SPARK_YARN_STAGING_DIR="wasbs://dhlsg-hd4insight-prod-clus@dhlsghdinsightprodsa.blob.core.windows.net/user/sshuser/.sparkStaging/application_1647400556153_0012"
export APP_SUBMIT_TIME_ENV="1647412638438"
export PYSPARK3_PYTHON="/usr/bin/anaconda/envs/py35/bin/python3"
export PYSPARK_PYTHON="/usr/bin/anaconda/bin/python"
export PYTHONPATH="/home/sshuser/pysqlite:/home/sshuser/dhl/aqcc/config:/usr/hdp/current/spark2-client/python:/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip:$PWD/pyspark.zip:$PWD/py4j-0.10.7-src.zip"
export APPLICATION_WEB_PROXY_BASE="/proxy/application_1647400556153_0012"
export SPARK_DIST_CLASSPATH=":/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:"
export CLASSPATH="$PWD:$PWD/__spark_conf__:$PWD/__spark_libs__/*:/usr/hdp/current/spark2-client/jars/*:$HADOOP_CONF_DIR:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*:/usr/hdp/current/hadoop-yarn-client/*:/usr/hdp/current/hadoop-yarn-client/lib/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/4.1.4.8/hadoop/lib/hadoop-lzo-0.6.0.4.1.4.8.jar:/etc/hadoop/conf/secure::/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf::$PWD/__spark_conf__/__hadoop_conf__"
export SPARK_USER="sshuser"
echo "Setting up job resources"
ln -sf "/mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21996/__spark_conf__.zip" "__spark_conf__"
ln -sf "/mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21997/py4j-0.10.7-src.zip" "py4j-0.10.7-src.zip"
ln -sf "/mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21995/pyspark.zip" "pyspark.zip"
ln -sf "/mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21998/__hive_libs__6588329932098669616.zip" "__hive_libs__"
echo "Copying debugging information"
# Creating copy of launch script
cp "launch_container.sh" "/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000001/launch_container.sh"
chmod 640 "/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000001/launch_container.sh"
# Determining directory contents
echo "ls -l:" 1>"/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000001/directory.info"
ls -l 1>>"/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000001/directory.info"
echo "find -L . -maxdepth 5 -ls:" 1>>"/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000001/directory.info"
find -L . -maxdepth 5 -ls 1>>"/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000001/directory.info"
echo "broken symlinks(find -L . -maxdepth 5 -type l -ls):" 1>>"/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000001/directory.info"
find -L . -maxdepth 5 -type l -ls 1>>"/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000001/directory.info"
echo "Launching container"
exec /bin/bash -c "$JAVA_HOME/bin/java -server -Xmx512m -Djava.io.tmpdir=$PWD/tmp -Dhdp.version=4.1.4.8 -Dspark.yarn.app.container.log.dir=/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000001 org.apache.spark.deploy.yarn.ExecutorLauncher --arg 'hn0-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net:45485' --properties-file $PWD/__spark_conf__/__spark_conf__.properties 1> /mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000001/stdout 2> /mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000001/stderr"

End of LogType:launch_container.sh
************************************************************************************


End of LogType:prelaunch.err
******************************************************************************

Container: container_e145_1647400556153_0012_01_000001 on wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net_30050_1647414834857
LogAggregationType: AGGREGATED
=============================================================================================================================================
LogType:prelaunch.out
LogLastModifiedTime:Wed Mar 16 07:13:54 +0000 2022
LogLength:100
LogContents:
Setting up env variables
Setting up job resources
Copying debugging information
Launching container

End of LogType:prelaunch.out
******************************************************************************

Container: container_e145_1647400556153_0012_01_000001 on wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net_30050_1647414834857
LogAggregationType: AGGREGATED
=============================================================================================================================================
LogType:stderr
LogLastModifiedTime:Wed Mar 16 07:13:54 +0000 2022
LogLength:752
LogContents:
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/hdp/4.1.4.8/spark2/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/4.1.4.8/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
22/03/16 06:37:24 WARN NativeCodeLoader [main]: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
22/03/16 07:13:52 WARN AzureFileSystemThreadPoolExecutor [shutdown-hook-0]: Disabling threads for Delete operation as thread count 0 is <= 1

End of LogType:stderr
***********************************************************************


End of LogType:stdout
***********************************************************************

Container: container_e145_1647400556153_0012_01_000002 on wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net_30050_1647414834857
LogAggregationType: AGGREGATED
=============================================================================================================================================
LogType:directory.info
LogLastModifiedTime:Wed Mar 16 07:13:54 +0000 2022
LogLength:7271
LogContents:
ls -l:
total 40
-rw-r--r-- 1 yarn hadoop  107 Mar 16 06:37 container_tokens
-rwx------ 1 yarn hadoop  714 Mar 16 06:37 default_container_executor_session.sh
-rwx------ 1 yarn hadoop  769 Mar 16 06:37 default_container_executor.sh
lrwxrwxrwx 1 yarn hadoop  102 Mar 16 06:37 __hive_libs__ -> /mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21998/__hive_libs__6588329932098669616.zip
-rwx------ 1 yarn hadoop 6860 Mar 16 06:37 launch_container.sh
lrwxrwxrwx 1 yarn hadoop   85 Mar 16 06:37 py4j-0.10.7-src.zip -> /mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21997/py4j-0.10.7-src.zip
lrwxrwxrwx 1 yarn hadoop   77 Mar 16 06:37 pyspark.zip -> /mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21995/pyspark.zip
lrwxrwxrwx 1 yarn hadoop   84 Mar 16 06:37 __spark_conf__ -> /mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21996/__spark_conf__.zip
drwx--x--- 2 yarn hadoop 4096 Mar 16 06:37 tmp
find -L . -maxdepth 5 -ls:
 31326213      4 drwx--x---   3 yarn     hadoop       4096 Mar 16 06:37 .
 31326247      4 -rw-r--r--   1 yarn     hadoop        107 Mar 16 06:37 ./container_tokens
 31326216      4 drwx--x---   2 yarn     hadoop       4096 Mar 16 06:37 ./tmp
 25824581    580 -r-x------   1 yarn     hadoop     592826 Mar 16 06:37 ./pyspark.zip
 31326254      4 -rw-r--r--   1 yarn     hadoop         64 Mar 16 06:37 ./.launch_container.sh.crc
 31326255      4 -rwx------   1 yarn     hadoop        714 Mar 16 06:37 ./default_container_executor_session.sh
 25824584      4 drwx------   3 yarn     hadoop       4096 Mar 16 06:37 ./__spark_conf__
 25824586      8 -r-x------   1 yarn     hadoop       5142 Mar 16 06:37 ./__spark_conf__/metrics.properties
 25824585      4 -r-x------   1 yarn     hadoop       3333 Mar 16 06:37 ./__spark_conf__/log4j.properties
 25824622    156 -r-x------   1 yarn     hadoop     157911 Mar 16 06:37 ./__spark_conf__/__spark_hadoop_conf__.xml
 25824587      4 drwx------   2 yarn     hadoop       4096 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__
 25824590      4 -r-x------   1 yarn     hadoop       2591 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/yarnservice-log4j.properties
 25824594     32 -r-x------   1 yarn     hadoop      29924 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/yarn-site.xml
 25824620      4 -r-x------   1 yarn     hadoop        945 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/taskcontroller.cfg
 25824609      4 -r-x------   1 yarn     hadoop        311 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/resource-types.xml
 25824618      4 -r-x------   1 yarn     hadoop       1044 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/ssl-server.xml
 25824605      8 -r-x------   1 yarn     hadoop       5296 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/yarn-env.sh
 25824621      8 -r-x------   1 yarn     hadoop       4113 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/mapred-queues.xml.template
 25824613      4 -r-x------   1 yarn     hadoop       1344 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hive-site.xml
 25824602      4 -r-x------   1 yarn     hadoop       2846 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/capacity-scheduler.xml
 25824593      4 -r-x------   1 yarn     hadoop        241 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hadoop-metrics2-adl-file-system.properties
 25824591     12 -r-x------   1 yarn     hadoop       8436 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/mapred-site.xml
 25824589     12 -r-x------   1 yarn     hadoop      10621 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/log4j.properties
 25824592      4 -r-x------   1 yarn     hadoop       2123 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hadoop-metrics2.properties
 25824604      4 -r-x------   1 yarn     hadoop       1335 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/configuration.xsl
 25824608      4 -r-x------   1 yarn     hadoop       1020 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/commons-logging.properties
 25824612      4 -r-x------   1 yarn     hadoop       1602 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/health_check
 25824615      4 -r-x------   1 yarn     hadoop       1352 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hadoop-policy.xml
 25824596      4 -r-x------   1 yarn     hadoop          1 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/yarn.exclude
 25824600      4 -r-x------   1 yarn     hadoop          1 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/dfs.exclude
 25824616      4 -r-x------   1 yarn     hadoop         65 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/slaves
 25824599      4 -r-x------   1 yarn     hadoop        244 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/spark-thrift-fairscheduler.xml
 25824607      4 -r-x------   1 yarn     hadoop       1674 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/container-executor.cfg
 25824588      8 -r-x------   1 yarn     hadoop       6195 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hadoop-env.sh
 25824614      4 -r-x------   1 yarn     hadoop       2316 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/ssl-client.xml.example
 25824606     12 -r-x------   1 yarn     hadoop       9223 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hdfs-site.xml
 25824595      4 -r-x------   1 yarn     hadoop        259 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hadoop-metrics2-azure-file-system.properties
 25824617      4 -r-x------   1 yarn     hadoop        122 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/topology_mappings.data
 25824619      4 -r-x------   1 yarn     hadoop       2697 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/ssl-server.xml.example
 25824611      4 -r-x------   1 yarn     hadoop       1282 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/mapred-env.sh
 25824601      4 -r-x------   1 yarn     hadoop        845 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/ssl-client.xml
 25824610      8 -r-x------   1 yarn     hadoop       4221 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/task-log4j.properties
 25824603      4 -r-x------   1 yarn     hadoop       2358 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/topology_script.py
 25824597     12 -r-x------   1 yarn     hadoop      11445 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/core-site.xml
 25824598      4 -r-x------   1 yarn     hadoop         10 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/workers
 25824623      8 -r-x------   1 yarn     hadoop       5017 Mar 16 06:37 ./__spark_conf__/__spark_conf__.properties
 25824628      4 drwx------   2 yarn     hadoop       4096 Mar 16 06:37 ./__hive_libs__
 25824629  48644 -r-x------   1 yarn     hadoop   49809546 Mar 16 06:37 ./__hive_libs__/standalone-metastore-1.21.4.4.1.4.8-hive3.jar
 25824625     44 -r-x------   1 yarn     hadoop      42437 Mar 16 06:37 ./py4j-0.10.7-src.zip
 31326258      4 -rw-r--r--   1 yarn     hadoop         16 Mar 16 06:37 ./.default_container_executor.sh.crc
 31326257      4 -rwx------   1 yarn     hadoop        769 Mar 16 06:37 ./default_container_executor.sh
 31326253      8 -rwx------   1 yarn     hadoop       6860 Mar 16 06:37 ./launch_container.sh
 31326248      4 -rw-r--r--   1 yarn     hadoop         12 Mar 16 06:37 ./.container_tokens.crc
 31326256      4 -rw-r--r--   1 yarn     hadoop         16 Mar 16 06:37 ./.default_container_executor_session.sh.crc
broken symlinks(find -L . -maxdepth 5 -type l -ls):

End of LogType:directory.info
*******************************************************************************

Container: container_e145_1647400556153_0012_01_000002 on wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net_30050_1647414834857
LogAggregationType: AGGREGATED
=============================================================================================================================================
LogType:launch_container.sh
LogLastModifiedTime:Wed Mar 16 07:13:54 +0000 2022
LogLength:6860
LogContents:
#!/bin/bash

set -o pipefail -e
export PRELAUNCH_OUT="/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000002/prelaunch.out"
exec >"${PRELAUNCH_OUT}"
export PRELAUNCH_ERR="/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000002/prelaunch.err"
exec 2>"${PRELAUNCH_ERR}"
echo "Setting up env variables"
export JAVA_HOME=${JAVA_HOME:-"/usr/lib/jvm/zulu-8-azure-amd64"}
export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-"/usr/hdp/4.1.4.8/hadoop/conf"}
export HADOOP_YARN_HOME=${HADOOP_YARN_HOME:-"/usr/hdp/4.1.4.8/hadoop-yarn"}
export HADOOP_HOME=${HADOOP_HOME:-"/usr/hdp/4.1.4.8/hadoop"}
export PATH=${PATH:-"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/var/lib/ambari-agent"}
export LANG=${LANG:-"en_US.UTF-8"}
export HADOOP_TOKEN_FILE_LOCATION="/mnt/resource/hadoop/yarn/local/usercache/sshuser/appcache/application_1647400556153_0012/container_e145_1647400556153_0012_01_000002/container_tokens"
export CONTAINER_ID="container_e145_1647400556153_0012_01_000002"
export NM_PORT="30050"
export NM_HOST="wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net"
export NM_HTTP_PORT="30060"
export LOCAL_DIRS="/mnt/resource/hadoop/yarn/local/usercache/sshuser/appcache/application_1647400556153_0012"
export LOCAL_USER_DIRS="/mnt/resource/hadoop/yarn/local/usercache/sshuser/"
export LOG_DIRS="/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000002"
export USER="sshuser"
export LOGNAME="sshuser"
export HOME="/home/"
export PWD="/mnt/resource/hadoop/yarn/local/usercache/sshuser/appcache/application_1647400556153_0012/container_e145_1647400556153_0012_01_000002"
export JVM_PID="$$"
export MALLOC_ARENA_MAX="4"
export NM_AUX_SERVICE_mapreduce_shuffle="AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="
export NM_AUX_SERVICE_spark2_shuffle=""
export SPARK_YARN_STAGING_DIR="wasbs://dhlsg-hd4insight-prod-clus@dhlsghdinsightprodsa.blob.core.windows.net/user/sshuser/.sparkStaging/application_1647400556153_0012"
export PYTHONPATH="/home/sshuser/pysqlite:/home/sshuser/dhl/aqcc/config:/usr/hdp/current/spark2-client/python:/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip:$PWD/pyspark.zip:$PWD/py4j-0.10.7-src.zip"
export SPARK_DIST_CLASSPATH=":/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:"
export CLASSPATH="$PWD:$PWD/__spark_conf__:$PWD/__spark_libs__/*:/usr/hdp/current/spark2-client/jars/*:$HADOOP_CONF_DIR:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*:/usr/hdp/current/hadoop-yarn-client/*:/usr/hdp/current/hadoop-yarn-client/lib/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/4.1.4.8/hadoop/lib/hadoop-lzo-0.6.0.4.1.4.8.jar:/etc/hadoop/conf/secure::/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf::$PWD/__spark_conf__/__hadoop_conf__"
export SPARK_LOG_URL_STDOUT="http://wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net:30060/node/containerlogs/container_e145_1647400556153_0012_01_000002/sshuser/stdout?start=-4096"
export SPARK_USER="sshuser"
export SPARK_LOG_URL_STDERR="http://wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net:30060/node/containerlogs/container_e145_1647400556153_0012_01_000002/sshuser/stderr?start=-4096"
echo "Setting up job resources"
ln -sf "/mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21996/__spark_conf__.zip" "__spark_conf__"
ln -sf "/mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21997/py4j-0.10.7-src.zip" "py4j-0.10.7-src.zip"
ln -sf "/mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21995/pyspark.zip" "pyspark.zip"
ln -sf "/mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21998/__hive_libs__6588329932098669616.zip" "__hive_libs__"
echo "Copying debugging information"
# Creating copy of launch script
cp "launch_container.sh" "/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000002/launch_container.sh"
chmod 640 "/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000002/launch_container.sh"
# Determining directory contents
echo "ls -l:" 1>"/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000002/directory.info"
ls -l 1>>"/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000002/directory.info"
echo "find -L . -maxdepth 5 -ls:" 1>>"/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000002/directory.info"
find -L . -maxdepth 5 -ls 1>>"/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000002/directory.info"
echo "broken symlinks(find -L . -maxdepth 5 -type l -ls):" 1>>"/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000002/directory.info"
find -L . -maxdepth 5 -type l -ls 1>>"/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000002/directory.info"
echo "Launching container"
exec /bin/bash -c "LD_LIBRARY_PATH=\"/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64:$LD_LIBRARY_PATH\" $JAVA_HOME/bin/java -server -Xmx49152m '-XX:+UseG1GC' -Djava.io.tmpdir=$PWD/tmp '-Dspark.network.timeout=2000s' '-Dspark.driver.port=45485' '-Dspark.history.ui.port=18080' -Dspark.yarn.app.container.log.dir=/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000002 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@hn0-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net:45485 --executor-id 1 --hostname wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net --cores 5 --app-id application_1647400556153_0012 --user-class-path file:$PWD/__app__.jar 1>/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000002/stdout 2>/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000002/stderr"

End of LogType:launch_container.sh
************************************************************************************


End of LogType:prelaunch.err
******************************************************************************

Container: container_e145_1647400556153_0012_01_000002 on wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net_30050_1647414834857
LogAggregationType: AGGREGATED
=============================================================================================================================================
LogType:prelaunch.out
LogLastModifiedTime:Wed Mar 16 07:13:54 +0000 2022
LogLength:100
LogContents:
Setting up env variables
Setting up job resources
Copying debugging information
Launching container

End of LogType:prelaunch.out
******************************************************************************

Container: container_e145_1647400556153_0012_01_000002 on wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net_30050_1647414834857
LogAggregationType: AGGREGATED
=============================================================================================================================================
LogType:stderr
LogLastModifiedTime:Wed Mar 16 07:13:54 +0000 2022
LogLength:6998
LogContents:
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/hdp/4.1.4.8/spark2/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/4.1.4.8/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
22/03/16 07:13:52 ERROR CoarseGrainedExecutorBackend [dispatcher-event-loop-3]: Executor self-exiting due to : Driver hn0-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net:45485 disassociated! Shutting down.
22/03/16 07:13:54 ERROR CoarseGrainedExecutorBackend [SIGTERM handler]: RECEIVED SIGNAL TERM
22/03/16 07:13:54 WARN Executor [driver-heartbeater]: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:92)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:841)
	at org.apache.spark.executor.Executor$$anon$2$$anonfun$run$1.apply$mcV$sp(Executor.scala:870)
	at org.apache.spark.executor.Executor$$anon$2$$anonfun$run$1.apply(Executor.scala:870)
	at org.apache.spark.executor.Executor$$anon$2$$anonfun$run$1.apply(Executor.scala:870)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.executor.Executor$$anon$2.run(Executor.scala:870)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to connect to hn0-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net/172.16.1.17:45485
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)
	at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:198)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:194)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:190)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hn0-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net/172.16.1.17:45485
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:688)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:635)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:552)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:514)
	at io.netty.util.concurrent.SingleThreadEventExecutor$6.run(SingleThreadEventExecutor.java:1044)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
22/03/16 07:13:54 WARN JavaUtils [shutdown-hook-0]: Attempt to delete using native Unix OS command failed for path = /mnt/resource/hadoop/yarn/local/usercache/sshuser/appcache/application_1647400556153_0012/blockmgr-1eafa396-ff7d-4873-b9ca-5cadb7cdc3f0. Falling back to Java IO way
java.io.IOException: Failed to delete: /mnt/resource/hadoop/yarn/local/usercache/sshuser/appcache/application_1647400556153_0012/blockmgr-1eafa396-ff7d-4873-b9ca-5cadb7cdc3f0
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:171)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:110)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1062)
	at org.apache.spark.storage.DiskBlockManager$$anonfun$org$apache$spark$storage$DiskBlockManager$$doStop$1.apply(DiskBlockManager.scala:178)
	at org.apache.spark.storage.DiskBlockManager$$anonfun$org$apache$spark$storage$DiskBlockManager$$doStop$1.apply(DiskBlockManager.scala:174)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.storage.DiskBlockManager.org$apache$spark$storage$DiskBlockManager$$doStop(DiskBlockManager.scala:174)
	at org.apache.spark.storage.DiskBlockManager$$anonfun$addShutdownHook$1.apply$mcV$sp(DiskBlockManager.scala:156)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

End of LogType:stderr
***********************************************************************


End of LogType:stdout
***********************************************************************

Container: container_e145_1647400556153_0012_01_000003 on wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net_30050_1647414834857
LogAggregationType: AGGREGATED
=============================================================================================================================================
LogType:directory.info
LogLastModifiedTime:Wed Mar 16 07:13:54 +0000 2022
LogLength:7271
LogContents:
ls -l:
total 40
-rw-r--r-- 1 yarn hadoop  107 Mar 16 06:37 container_tokens
-rwx------ 1 yarn hadoop  714 Mar 16 06:37 default_container_executor_session.sh
-rwx------ 1 yarn hadoop  769 Mar 16 06:37 default_container_executor.sh
lrwxrwxrwx 1 yarn hadoop  102 Mar 16 06:37 __hive_libs__ -> /mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21998/__hive_libs__6588329932098669616.zip
-rwx------ 1 yarn hadoop 6860 Mar 16 06:37 launch_container.sh
lrwxrwxrwx 1 yarn hadoop   85 Mar 16 06:37 py4j-0.10.7-src.zip -> /mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21997/py4j-0.10.7-src.zip
lrwxrwxrwx 1 yarn hadoop   77 Mar 16 06:37 pyspark.zip -> /mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21995/pyspark.zip
lrwxrwxrwx 1 yarn hadoop   84 Mar 16 06:37 __spark_conf__ -> /mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21996/__spark_conf__.zip
drwx--x--- 2 yarn hadoop 4096 Mar 16 06:37 tmp
find -L . -maxdepth 5 -ls:
 31326289      4 drwx--x---   3 yarn     hadoop       4096 Mar 16 06:37 .
 31326293      4 -rw-r--r--   1 yarn     hadoop        107 Mar 16 06:37 ./container_tokens
 31326290      4 drwx--x---   2 yarn     hadoop       4096 Mar 16 06:37 ./tmp
 25824581    580 -r-x------   1 yarn     hadoop     592826 Mar 16 06:37 ./pyspark.zip
 31326298      4 -rw-r--r--   1 yarn     hadoop         64 Mar 16 06:37 ./.launch_container.sh.crc
 31326299      4 -rwx------   1 yarn     hadoop        714 Mar 16 06:37 ./default_container_executor_session.sh
 25824584      4 drwx------   3 yarn     hadoop       4096 Mar 16 06:37 ./__spark_conf__
 25824586      8 -r-x------   1 yarn     hadoop       5142 Mar 16 06:37 ./__spark_conf__/metrics.properties
 25824585      4 -r-x------   1 yarn     hadoop       3333 Mar 16 06:37 ./__spark_conf__/log4j.properties
 25824622    156 -r-x------   1 yarn     hadoop     157911 Mar 16 06:37 ./__spark_conf__/__spark_hadoop_conf__.xml
 25824587      4 drwx------   2 yarn     hadoop       4096 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__
 25824590      4 -r-x------   1 yarn     hadoop       2591 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/yarnservice-log4j.properties
 25824594     32 -r-x------   1 yarn     hadoop      29924 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/yarn-site.xml
 25824620      4 -r-x------   1 yarn     hadoop        945 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/taskcontroller.cfg
 25824609      4 -r-x------   1 yarn     hadoop        311 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/resource-types.xml
 25824618      4 -r-x------   1 yarn     hadoop       1044 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/ssl-server.xml
 25824605      8 -r-x------   1 yarn     hadoop       5296 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/yarn-env.sh
 25824621      8 -r-x------   1 yarn     hadoop       4113 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/mapred-queues.xml.template
 25824613      4 -r-x------   1 yarn     hadoop       1344 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hive-site.xml
 25824602      4 -r-x------   1 yarn     hadoop       2846 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/capacity-scheduler.xml
 25824593      4 -r-x------   1 yarn     hadoop        241 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hadoop-metrics2-adl-file-system.properties
 25824591     12 -r-x------   1 yarn     hadoop       8436 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/mapred-site.xml
 25824589     12 -r-x------   1 yarn     hadoop      10621 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/log4j.properties
 25824592      4 -r-x------   1 yarn     hadoop       2123 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hadoop-metrics2.properties
 25824604      4 -r-x------   1 yarn     hadoop       1335 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/configuration.xsl
 25824608      4 -r-x------   1 yarn     hadoop       1020 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/commons-logging.properties
 25824612      4 -r-x------   1 yarn     hadoop       1602 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/health_check
 25824615      4 -r-x------   1 yarn     hadoop       1352 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hadoop-policy.xml
 25824596      4 -r-x------   1 yarn     hadoop          1 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/yarn.exclude
 25824600      4 -r-x------   1 yarn     hadoop          1 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/dfs.exclude
 25824616      4 -r-x------   1 yarn     hadoop         65 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/slaves
 25824599      4 -r-x------   1 yarn     hadoop        244 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/spark-thrift-fairscheduler.xml
 25824607      4 -r-x------   1 yarn     hadoop       1674 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/container-executor.cfg
 25824588      8 -r-x------   1 yarn     hadoop       6195 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hadoop-env.sh
 25824614      4 -r-x------   1 yarn     hadoop       2316 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/ssl-client.xml.example
 25824606     12 -r-x------   1 yarn     hadoop       9223 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hdfs-site.xml
 25824595      4 -r-x------   1 yarn     hadoop        259 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hadoop-metrics2-azure-file-system.properties
 25824617      4 -r-x------   1 yarn     hadoop        122 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/topology_mappings.data
 25824619      4 -r-x------   1 yarn     hadoop       2697 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/ssl-server.xml.example
 25824611      4 -r-x------   1 yarn     hadoop       1282 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/mapred-env.sh
 25824601      4 -r-x------   1 yarn     hadoop        845 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/ssl-client.xml
 25824610      8 -r-x------   1 yarn     hadoop       4221 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/task-log4j.properties
 25824603      4 -r-x------   1 yarn     hadoop       2358 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/topology_script.py
 25824597     12 -r-x------   1 yarn     hadoop      11445 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/core-site.xml
 25824598      4 -r-x------   1 yarn     hadoop         10 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/workers
 25824623      8 -r-x------   1 yarn     hadoop       5017 Mar 16 06:37 ./__spark_conf__/__spark_conf__.properties
 25824628      4 drwx------   2 yarn     hadoop       4096 Mar 16 06:37 ./__hive_libs__
 25824629  48644 -r-x------   1 yarn     hadoop   49809546 Mar 16 06:37 ./__hive_libs__/standalone-metastore-1.21.4.4.1.4.8-hive3.jar
 25824625     44 -r-x------   1 yarn     hadoop      42437 Mar 16 06:37 ./py4j-0.10.7-src.zip
 31326310      4 -rw-r--r--   1 yarn     hadoop         16 Mar 16 06:37 ./.default_container_executor.sh.crc
 31326309      4 -rwx------   1 yarn     hadoop        769 Mar 16 06:37 ./default_container_executor.sh
 31326297      8 -rwx------   1 yarn     hadoop       6860 Mar 16 06:37 ./launch_container.sh
 31326294      4 -rw-r--r--   1 yarn     hadoop         12 Mar 16 06:37 ./.container_tokens.crc
 31326300      4 -rw-r--r--   1 yarn     hadoop         16 Mar 16 06:37 ./.default_container_executor_session.sh.crc
broken symlinks(find -L . -maxdepth 5 -type l -ls):

End of LogType:directory.info
*******************************************************************************

Container: container_e145_1647400556153_0012_01_000003 on wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net_30050_1647414834857
LogAggregationType: AGGREGATED
=============================================================================================================================================
LogType:launch_container.sh
LogLastModifiedTime:Wed Mar 16 07:13:54 +0000 2022
LogLength:6860
LogContents:
#!/bin/bash

set -o pipefail -e
export PRELAUNCH_OUT="/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000003/prelaunch.out"
exec >"${PRELAUNCH_OUT}"
export PRELAUNCH_ERR="/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000003/prelaunch.err"
exec 2>"${PRELAUNCH_ERR}"
echo "Setting up env variables"
export JAVA_HOME=${JAVA_HOME:-"/usr/lib/jvm/zulu-8-azure-amd64"}
export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-"/usr/hdp/4.1.4.8/hadoop/conf"}
export HADOOP_YARN_HOME=${HADOOP_YARN_HOME:-"/usr/hdp/4.1.4.8/hadoop-yarn"}
export HADOOP_HOME=${HADOOP_HOME:-"/usr/hdp/4.1.4.8/hadoop"}
export PATH=${PATH:-"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/var/lib/ambari-agent"}
export LANG=${LANG:-"en_US.UTF-8"}
export HADOOP_TOKEN_FILE_LOCATION="/mnt/resource/hadoop/yarn/local/usercache/sshuser/appcache/application_1647400556153_0012/container_e145_1647400556153_0012_01_000003/container_tokens"
export CONTAINER_ID="container_e145_1647400556153_0012_01_000003"
export NM_PORT="30050"
export NM_HOST="wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net"
export NM_HTTP_PORT="30060"
export LOCAL_DIRS="/mnt/resource/hadoop/yarn/local/usercache/sshuser/appcache/application_1647400556153_0012"
export LOCAL_USER_DIRS="/mnt/resource/hadoop/yarn/local/usercache/sshuser/"
export LOG_DIRS="/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000003"
export USER="sshuser"
export LOGNAME="sshuser"
export HOME="/home/"
export PWD="/mnt/resource/hadoop/yarn/local/usercache/sshuser/appcache/application_1647400556153_0012/container_e145_1647400556153_0012_01_000003"
export JVM_PID="$$"
export MALLOC_ARENA_MAX="4"
export NM_AUX_SERVICE_mapreduce_shuffle="AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="
export NM_AUX_SERVICE_spark2_shuffle=""
export SPARK_YARN_STAGING_DIR="wasbs://dhlsg-hd4insight-prod-clus@dhlsghdinsightprodsa.blob.core.windows.net/user/sshuser/.sparkStaging/application_1647400556153_0012"
export PYTHONPATH="/home/sshuser/pysqlite:/home/sshuser/dhl/aqcc/config:/usr/hdp/current/spark2-client/python:/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip:$PWD/pyspark.zip:$PWD/py4j-0.10.7-src.zip"
export SPARK_DIST_CLASSPATH=":/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:"
export CLASSPATH="$PWD:$PWD/__spark_conf__:$PWD/__spark_libs__/*:/usr/hdp/current/spark2-client/jars/*:$HADOOP_CONF_DIR:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*:/usr/hdp/current/hadoop-yarn-client/*:/usr/hdp/current/hadoop-yarn-client/lib/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/4.1.4.8/hadoop/lib/hadoop-lzo-0.6.0.4.1.4.8.jar:/etc/hadoop/conf/secure::/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf::$PWD/__spark_conf__/__hadoop_conf__"
export SPARK_LOG_URL_STDOUT="http://wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net:30060/node/containerlogs/container_e145_1647400556153_0012_01_000003/sshuser/stdout?start=-4096"
export SPARK_USER="sshuser"
export SPARK_LOG_URL_STDERR="http://wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net:30060/node/containerlogs/container_e145_1647400556153_0012_01_000003/sshuser/stderr?start=-4096"
echo "Setting up job resources"
ln -sf "/mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21996/__spark_conf__.zip" "__spark_conf__"
ln -sf "/mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21997/py4j-0.10.7-src.zip" "py4j-0.10.7-src.zip"
ln -sf "/mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21995/pyspark.zip" "pyspark.zip"
ln -sf "/mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21998/__hive_libs__6588329932098669616.zip" "__hive_libs__"
echo "Copying debugging information"
# Creating copy of launch script
cp "launch_container.sh" "/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000003/launch_container.sh"
chmod 640 "/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000003/launch_container.sh"
# Determining directory contents
echo "ls -l:" 1>"/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000003/directory.info"
ls -l 1>>"/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000003/directory.info"
echo "find -L . -maxdepth 5 -ls:" 1>>"/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000003/directory.info"
find -L . -maxdepth 5 -ls 1>>"/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000003/directory.info"
echo "broken symlinks(find -L . -maxdepth 5 -type l -ls):" 1>>"/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000003/directory.info"
find -L . -maxdepth 5 -type l -ls 1>>"/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000003/directory.info"
echo "Launching container"
exec /bin/bash -c "LD_LIBRARY_PATH=\"/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64:$LD_LIBRARY_PATH\" $JAVA_HOME/bin/java -server -Xmx49152m '-XX:+UseG1GC' -Djava.io.tmpdir=$PWD/tmp '-Dspark.network.timeout=2000s' '-Dspark.driver.port=45485' '-Dspark.history.ui.port=18080' -Dspark.yarn.app.container.log.dir=/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000003 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@hn0-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net:45485 --executor-id 2 --hostname wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net --cores 5 --app-id application_1647400556153_0012 --user-class-path file:$PWD/__app__.jar 1>/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000003/stdout 2>/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000003/stderr"

End of LogType:launch_container.sh
************************************************************************************


End of LogType:prelaunch.err
******************************************************************************

Container: container_e145_1647400556153_0012_01_000003 on wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net_30050_1647414834857
LogAggregationType: AGGREGATED
=============================================================================================================================================
LogType:prelaunch.out
LogLastModifiedTime:Wed Mar 16 07:13:54 +0000 2022
LogLength:100
LogContents:
Setting up env variables
Setting up job resources
Copying debugging information
Launching container

End of LogType:prelaunch.out
******************************************************************************

Container: container_e145_1647400556153_0012_01_000003 on wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net_30050_1647414834857
LogAggregationType: AGGREGATED
=============================================================================================================================================
LogType:stderr
LogLastModifiedTime:Wed Mar 16 07:13:54 +0000 2022
LogLength:6998
LogContents:
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/hdp/4.1.4.8/spark2/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/4.1.4.8/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
22/03/16 07:13:52 ERROR CoarseGrainedExecutorBackend [dispatcher-event-loop-3]: Executor self-exiting due to : Driver hn0-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net:45485 disassociated! Shutting down.
22/03/16 07:13:53 WARN Executor [driver-heartbeater]: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:92)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:841)
	at org.apache.spark.executor.Executor$$anon$2$$anonfun$run$1.apply$mcV$sp(Executor.scala:870)
	at org.apache.spark.executor.Executor$$anon$2$$anonfun$run$1.apply(Executor.scala:870)
	at org.apache.spark.executor.Executor$$anon$2$$anonfun$run$1.apply(Executor.scala:870)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.executor.Executor$$anon$2.run(Executor.scala:870)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to connect to hn0-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net/172.16.1.17:45485
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)
	at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:198)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:194)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:190)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hn0-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net/172.16.1.17:45485
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:688)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:635)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:552)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:514)
	at io.netty.util.concurrent.SingleThreadEventExecutor$6.run(SingleThreadEventExecutor.java:1044)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
22/03/16 07:13:54 ERROR CoarseGrainedExecutorBackend [SIGTERM handler]: RECEIVED SIGNAL TERM
22/03/16 07:13:54 WARN JavaUtils [shutdown-hook-0]: Attempt to delete using native Unix OS command failed for path = /mnt/resource/hadoop/yarn/local/usercache/sshuser/appcache/application_1647400556153_0012/blockmgr-39a3bb64-5c6e-41fc-8ffe-7944751339a4. Falling back to Java IO way
java.io.IOException: Failed to delete: /mnt/resource/hadoop/yarn/local/usercache/sshuser/appcache/application_1647400556153_0012/blockmgr-39a3bb64-5c6e-41fc-8ffe-7944751339a4
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:171)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:110)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1062)
	at org.apache.spark.storage.DiskBlockManager$$anonfun$org$apache$spark$storage$DiskBlockManager$$doStop$1.apply(DiskBlockManager.scala:178)
	at org.apache.spark.storage.DiskBlockManager$$anonfun$org$apache$spark$storage$DiskBlockManager$$doStop$1.apply(DiskBlockManager.scala:174)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.storage.DiskBlockManager.org$apache$spark$storage$DiskBlockManager$$doStop(DiskBlockManager.scala:174)
	at org.apache.spark.storage.DiskBlockManager$$anonfun$addShutdownHook$1.apply$mcV$sp(DiskBlockManager.scala:156)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

End of LogType:stderr
***********************************************************************


End of LogType:stdout
***********************************************************************

Container: container_e145_1647400556153_0012_01_000004 on wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net_30050_1647414834857
LogAggregationType: AGGREGATED
=============================================================================================================================================
LogType:directory.info
LogLastModifiedTime:Wed Mar 16 07:13:54 +0000 2022
LogLength:7271
LogContents:
ls -l:
total 40
-rw-r--r-- 1 yarn hadoop  107 Mar 16 06:37 container_tokens
-rwx------ 1 yarn hadoop  714 Mar 16 06:37 default_container_executor_session.sh
-rwx------ 1 yarn hadoop  769 Mar 16 06:37 default_container_executor.sh
lrwxrwxrwx 1 yarn hadoop  102 Mar 16 06:37 __hive_libs__ -> /mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21998/__hive_libs__6588329932098669616.zip
-rwx------ 1 yarn hadoop 6860 Mar 16 06:37 launch_container.sh
lrwxrwxrwx 1 yarn hadoop   85 Mar 16 06:37 py4j-0.10.7-src.zip -> /mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21997/py4j-0.10.7-src.zip
lrwxrwxrwx 1 yarn hadoop   77 Mar 16 06:37 pyspark.zip -> /mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21995/pyspark.zip
lrwxrwxrwx 1 yarn hadoop   84 Mar 16 06:37 __spark_conf__ -> /mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21996/__spark_conf__.zip
drwx--x--- 2 yarn hadoop 4096 Mar 16 06:37 tmp
find -L . -maxdepth 5 -ls:
 31326273      4 drwx--x---   3 yarn     hadoop       4096 Mar 16 06:37 .
 31326275      4 -rw-r--r--   1 yarn     hadoop        107 Mar 16 06:37 ./container_tokens
 31326274      4 drwx--x---   2 yarn     hadoop       4096 Mar 16 06:37 ./tmp
 25824581    580 -r-x------   1 yarn     hadoop     592826 Mar 16 06:37 ./pyspark.zip
 31326282      4 -rw-r--r--   1 yarn     hadoop         64 Mar 16 06:37 ./.launch_container.sh.crc
 31326283      4 -rwx------   1 yarn     hadoop        714 Mar 16 06:37 ./default_container_executor_session.sh
 25824584      4 drwx------   3 yarn     hadoop       4096 Mar 16 06:37 ./__spark_conf__
 25824586      8 -r-x------   1 yarn     hadoop       5142 Mar 16 06:37 ./__spark_conf__/metrics.properties
 25824585      4 -r-x------   1 yarn     hadoop       3333 Mar 16 06:37 ./__spark_conf__/log4j.properties
 25824622    156 -r-x------   1 yarn     hadoop     157911 Mar 16 06:37 ./__spark_conf__/__spark_hadoop_conf__.xml
 25824587      4 drwx------   2 yarn     hadoop       4096 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__
 25824590      4 -r-x------   1 yarn     hadoop       2591 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/yarnservice-log4j.properties
 25824594     32 -r-x------   1 yarn     hadoop      29924 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/yarn-site.xml
 25824620      4 -r-x------   1 yarn     hadoop        945 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/taskcontroller.cfg
 25824609      4 -r-x------   1 yarn     hadoop        311 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/resource-types.xml
 25824618      4 -r-x------   1 yarn     hadoop       1044 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/ssl-server.xml
 25824605      8 -r-x------   1 yarn     hadoop       5296 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/yarn-env.sh
 25824621      8 -r-x------   1 yarn     hadoop       4113 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/mapred-queues.xml.template
 25824613      4 -r-x------   1 yarn     hadoop       1344 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hive-site.xml
 25824602      4 -r-x------   1 yarn     hadoop       2846 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/capacity-scheduler.xml
 25824593      4 -r-x------   1 yarn     hadoop        241 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hadoop-metrics2-adl-file-system.properties
 25824591     12 -r-x------   1 yarn     hadoop       8436 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/mapred-site.xml
 25824589     12 -r-x------   1 yarn     hadoop      10621 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/log4j.properties
 25824592      4 -r-x------   1 yarn     hadoop       2123 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hadoop-metrics2.properties
 25824604      4 -r-x------   1 yarn     hadoop       1335 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/configuration.xsl
 25824608      4 -r-x------   1 yarn     hadoop       1020 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/commons-logging.properties
 25824612      4 -r-x------   1 yarn     hadoop       1602 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/health_check
 25824615      4 -r-x------   1 yarn     hadoop       1352 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hadoop-policy.xml
 25824596      4 -r-x------   1 yarn     hadoop          1 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/yarn.exclude
 25824600      4 -r-x------   1 yarn     hadoop          1 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/dfs.exclude
 25824616      4 -r-x------   1 yarn     hadoop         65 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/slaves
 25824599      4 -r-x------   1 yarn     hadoop        244 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/spark-thrift-fairscheduler.xml
 25824607      4 -r-x------   1 yarn     hadoop       1674 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/container-executor.cfg
 25824588      8 -r-x------   1 yarn     hadoop       6195 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hadoop-env.sh
 25824614      4 -r-x------   1 yarn     hadoop       2316 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/ssl-client.xml.example
 25824606     12 -r-x------   1 yarn     hadoop       9223 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hdfs-site.xml
 25824595      4 -r-x------   1 yarn     hadoop        259 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hadoop-metrics2-azure-file-system.properties
 25824617      4 -r-x------   1 yarn     hadoop        122 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/topology_mappings.data
 25824619      4 -r-x------   1 yarn     hadoop       2697 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/ssl-server.xml.example
 25824611      4 -r-x------   1 yarn     hadoop       1282 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/mapred-env.sh
 25824601      4 -r-x------   1 yarn     hadoop        845 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/ssl-client.xml
 25824610      8 -r-x------   1 yarn     hadoop       4221 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/task-log4j.properties
 25824603      4 -r-x------   1 yarn     hadoop       2358 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/topology_script.py
 25824597     12 -r-x------   1 yarn     hadoop      11445 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/core-site.xml
 25824598      4 -r-x------   1 yarn     hadoop         10 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/workers
 25824623      8 -r-x------   1 yarn     hadoop       5017 Mar 16 06:37 ./__spark_conf__/__spark_conf__.properties
 25824628      4 drwx------   2 yarn     hadoop       4096 Mar 16 06:37 ./__hive_libs__
 25824629  48644 -r-x------   1 yarn     hadoop   49809546 Mar 16 06:37 ./__hive_libs__/standalone-metastore-1.21.4.4.1.4.8-hive3.jar
 25824625     44 -r-x------   1 yarn     hadoop      42437 Mar 16 06:37 ./py4j-0.10.7-src.zip
 31326288      4 -rw-r--r--   1 yarn     hadoop         16 Mar 16 06:37 ./.default_container_executor.sh.crc
 31326287      4 -rwx------   1 yarn     hadoop        769 Mar 16 06:37 ./default_container_executor.sh
 31326281      8 -rwx------   1 yarn     hadoop       6860 Mar 16 06:37 ./launch_container.sh
 31326276      4 -rw-r--r--   1 yarn     hadoop         12 Mar 16 06:37 ./.container_tokens.crc
 31326284      4 -rw-r--r--   1 yarn     hadoop         16 Mar 16 06:37 ./.default_container_executor_session.sh.crc
broken symlinks(find -L . -maxdepth 5 -type l -ls):

End of LogType:directory.info
*******************************************************************************

Container: container_e145_1647400556153_0012_01_000004 on wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net_30050_1647414834857
LogAggregationType: AGGREGATED
=============================================================================================================================================
LogType:launch_container.sh
LogLastModifiedTime:Wed Mar 16 07:13:54 +0000 2022
LogLength:6860
LogContents:
#!/bin/bash

set -o pipefail -e
export PRELAUNCH_OUT="/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000004/prelaunch.out"
exec >"${PRELAUNCH_OUT}"
export PRELAUNCH_ERR="/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000004/prelaunch.err"
exec 2>"${PRELAUNCH_ERR}"
echo "Setting up env variables"
export JAVA_HOME=${JAVA_HOME:-"/usr/lib/jvm/zulu-8-azure-amd64"}
export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-"/usr/hdp/4.1.4.8/hadoop/conf"}
export HADOOP_YARN_HOME=${HADOOP_YARN_HOME:-"/usr/hdp/4.1.4.8/hadoop-yarn"}
export HADOOP_HOME=${HADOOP_HOME:-"/usr/hdp/4.1.4.8/hadoop"}
export PATH=${PATH:-"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/var/lib/ambari-agent"}
export LANG=${LANG:-"en_US.UTF-8"}
export HADOOP_TOKEN_FILE_LOCATION="/mnt/resource/hadoop/yarn/local/usercache/sshuser/appcache/application_1647400556153_0012/container_e145_1647400556153_0012_01_000004/container_tokens"
export CONTAINER_ID="container_e145_1647400556153_0012_01_000004"
export NM_PORT="30050"
export NM_HOST="wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net"
export NM_HTTP_PORT="30060"
export LOCAL_DIRS="/mnt/resource/hadoop/yarn/local/usercache/sshuser/appcache/application_1647400556153_0012"
export LOCAL_USER_DIRS="/mnt/resource/hadoop/yarn/local/usercache/sshuser/"
export LOG_DIRS="/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000004"
export USER="sshuser"
export LOGNAME="sshuser"
export HOME="/home/"
export PWD="/mnt/resource/hadoop/yarn/local/usercache/sshuser/appcache/application_1647400556153_0012/container_e145_1647400556153_0012_01_000004"
export JVM_PID="$$"
export MALLOC_ARENA_MAX="4"
export NM_AUX_SERVICE_mapreduce_shuffle="AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="
export NM_AUX_SERVICE_spark2_shuffle=""
export SPARK_YARN_STAGING_DIR="wasbs://dhlsg-hd4insight-prod-clus@dhlsghdinsightprodsa.blob.core.windows.net/user/sshuser/.sparkStaging/application_1647400556153_0012"
export PYTHONPATH="/home/sshuser/pysqlite:/home/sshuser/dhl/aqcc/config:/usr/hdp/current/spark2-client/python:/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip:$PWD/pyspark.zip:$PWD/py4j-0.10.7-src.zip"
export SPARK_DIST_CLASSPATH=":/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:"
export CLASSPATH="$PWD:$PWD/__spark_conf__:$PWD/__spark_libs__/*:/usr/hdp/current/spark2-client/jars/*:$HADOOP_CONF_DIR:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*:/usr/hdp/current/hadoop-yarn-client/*:/usr/hdp/current/hadoop-yarn-client/lib/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/4.1.4.8/hadoop/lib/hadoop-lzo-0.6.0.4.1.4.8.jar:/etc/hadoop/conf/secure::/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf::$PWD/__spark_conf__/__hadoop_conf__"
export SPARK_LOG_URL_STDOUT="http://wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net:30060/node/containerlogs/container_e145_1647400556153_0012_01_000004/sshuser/stdout?start=-4096"
export SPARK_USER="sshuser"
export SPARK_LOG_URL_STDERR="http://wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net:30060/node/containerlogs/container_e145_1647400556153_0012_01_000004/sshuser/stderr?start=-4096"
echo "Setting up job resources"
ln -sf "/mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21996/__spark_conf__.zip" "__spark_conf__"
ln -sf "/mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21997/py4j-0.10.7-src.zip" "py4j-0.10.7-src.zip"
ln -sf "/mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21995/pyspark.zip" "pyspark.zip"
ln -sf "/mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21998/__hive_libs__6588329932098669616.zip" "__hive_libs__"
echo "Copying debugging information"
# Creating copy of launch script
cp "launch_container.sh" "/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000004/launch_container.sh"
chmod 640 "/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000004/launch_container.sh"
# Determining directory contents
echo "ls -l:" 1>"/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000004/directory.info"
ls -l 1>>"/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000004/directory.info"
echo "find -L . -maxdepth 5 -ls:" 1>>"/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000004/directory.info"
find -L . -maxdepth 5 -ls 1>>"/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000004/directory.info"
echo "broken symlinks(find -L . -maxdepth 5 -type l -ls):" 1>>"/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000004/directory.info"
find -L . -maxdepth 5 -type l -ls 1>>"/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000004/directory.info"
echo "Launching container"
exec /bin/bash -c "LD_LIBRARY_PATH=\"/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64:$LD_LIBRARY_PATH\" $JAVA_HOME/bin/java -server -Xmx49152m '-XX:+UseG1GC' -Djava.io.tmpdir=$PWD/tmp '-Dspark.network.timeout=2000s' '-Dspark.driver.port=45485' '-Dspark.history.ui.port=18080' -Dspark.yarn.app.container.log.dir=/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000004 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@hn0-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net:45485 --executor-id 3 --hostname wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net --cores 5 --app-id application_1647400556153_0012 --user-class-path file:$PWD/__app__.jar 1>/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000004/stdout 2>/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000004/stderr"

End of LogType:launch_container.sh
************************************************************************************


End of LogType:prelaunch.err
******************************************************************************

Container: container_e145_1647400556153_0012_01_000004 on wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net_30050_1647414834857
LogAggregationType: AGGREGATED
=============================================================================================================================================
LogType:prelaunch.out
LogLastModifiedTime:Wed Mar 16 07:13:54 +0000 2022
LogLength:100
LogContents:
Setting up env variables
Setting up job resources
Copying debugging information
Launching container

End of LogType:prelaunch.out
******************************************************************************

Container: container_e145_1647400556153_0012_01_000004 on wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net_30050_1647414834857
LogAggregationType: AGGREGATED
=============================================================================================================================================
LogType:stderr
LogLastModifiedTime:Wed Mar 16 07:13:54 +0000 2022
LogLength:3678
LogContents:
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/hdp/4.1.4.8/spark2/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/4.1.4.8/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
22/03/16 07:13:52 ERROR CoarseGrainedExecutorBackend [dispatcher-event-loop-4]: Executor self-exiting due to : Driver hn0-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net:45485 disassociated! Shutting down.
22/03/16 07:13:54 ERROR CoarseGrainedExecutorBackend [SIGTERM handler]: RECEIVED SIGNAL TERM
22/03/16 07:13:54 WARN JavaUtils [shutdown-hook-0]: Attempt to delete using native Unix OS command failed for path = /mnt/resource/hadoop/yarn/local/usercache/sshuser/appcache/application_1647400556153_0012/blockmgr-c1adb29f-e290-4b9d-8dab-8d31bf32c92b. Falling back to Java IO way
java.io.IOException: Failed to delete: /mnt/resource/hadoop/yarn/local/usercache/sshuser/appcache/application_1647400556153_0012/blockmgr-c1adb29f-e290-4b9d-8dab-8d31bf32c92b
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:171)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:110)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1062)
	at org.apache.spark.storage.DiskBlockManager$$anonfun$org$apache$spark$storage$DiskBlockManager$$doStop$1.apply(DiskBlockManager.scala:178)
	at org.apache.spark.storage.DiskBlockManager$$anonfun$org$apache$spark$storage$DiskBlockManager$$doStop$1.apply(DiskBlockManager.scala:174)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.storage.DiskBlockManager.org$apache$spark$storage$DiskBlockManager$$doStop(DiskBlockManager.scala:174)
	at org.apache.spark.storage.DiskBlockManager$$anonfun$addShutdownHook$1.apply$mcV$sp(DiskBlockManager.scala:156)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

End of LogType:stderr
***********************************************************************


End of LogType:stdout
***********************************************************************

Container: container_e145_1647400556153_0012_01_000005 on wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net_30050_1647414834857
LogAggregationType: AGGREGATED
=============================================================================================================================================
LogType:directory.info
LogLastModifiedTime:Wed Mar 16 07:13:54 +0000 2022
LogLength:7271
LogContents:
ls -l:
total 40
-rw-r--r-- 1 yarn hadoop  107 Mar 16 06:37 container_tokens
-rwx------ 1 yarn hadoop  714 Mar 16 06:37 default_container_executor_session.sh
-rwx------ 1 yarn hadoop  769 Mar 16 06:37 default_container_executor.sh
lrwxrwxrwx 1 yarn hadoop  102 Mar 16 06:37 __hive_libs__ -> /mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21998/__hive_libs__6588329932098669616.zip
-rwx------ 1 yarn hadoop 6860 Mar 16 06:37 launch_container.sh
lrwxrwxrwx 1 yarn hadoop   85 Mar 16 06:37 py4j-0.10.7-src.zip -> /mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21997/py4j-0.10.7-src.zip
lrwxrwxrwx 1 yarn hadoop   77 Mar 16 06:37 pyspark.zip -> /mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21995/pyspark.zip
lrwxrwxrwx 1 yarn hadoop   84 Mar 16 06:37 __spark_conf__ -> /mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21996/__spark_conf__.zip
drwx--x--- 2 yarn hadoop 4096 Mar 16 06:37 tmp
find -L . -maxdepth 5 -ls:
 31326320      4 drwx--x---   3 yarn     hadoop       4096 Mar 16 06:37 .
 31326322      4 -rw-r--r--   1 yarn     hadoop        107 Mar 16 06:37 ./container_tokens
 31326321      4 drwx--x---   2 yarn     hadoop       4096 Mar 16 06:37 ./tmp
 25824581    580 -r-x------   1 yarn     hadoop     592826 Mar 16 06:37 ./pyspark.zip
 31326325      4 -rw-r--r--   1 yarn     hadoop         64 Mar 16 06:37 ./.launch_container.sh.crc
 31326326      4 -rwx------   1 yarn     hadoop        714 Mar 16 06:37 ./default_container_executor_session.sh
 25824584      4 drwx------   3 yarn     hadoop       4096 Mar 16 06:37 ./__spark_conf__
 25824586      8 -r-x------   1 yarn     hadoop       5142 Mar 16 06:37 ./__spark_conf__/metrics.properties
 25824585      4 -r-x------   1 yarn     hadoop       3333 Mar 16 06:37 ./__spark_conf__/log4j.properties
 25824622    156 -r-x------   1 yarn     hadoop     157911 Mar 16 06:37 ./__spark_conf__/__spark_hadoop_conf__.xml
 25824587      4 drwx------   2 yarn     hadoop       4096 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__
 25824590      4 -r-x------   1 yarn     hadoop       2591 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/yarnservice-log4j.properties
 25824594     32 -r-x------   1 yarn     hadoop      29924 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/yarn-site.xml
 25824620      4 -r-x------   1 yarn     hadoop        945 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/taskcontroller.cfg
 25824609      4 -r-x------   1 yarn     hadoop        311 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/resource-types.xml
 25824618      4 -r-x------   1 yarn     hadoop       1044 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/ssl-server.xml
 25824605      8 -r-x------   1 yarn     hadoop       5296 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/yarn-env.sh
 25824621      8 -r-x------   1 yarn     hadoop       4113 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/mapred-queues.xml.template
 25824613      4 -r-x------   1 yarn     hadoop       1344 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hive-site.xml
 25824602      4 -r-x------   1 yarn     hadoop       2846 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/capacity-scheduler.xml
 25824593      4 -r-x------   1 yarn     hadoop        241 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hadoop-metrics2-adl-file-system.properties
 25824591     12 -r-x------   1 yarn     hadoop       8436 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/mapred-site.xml
 25824589     12 -r-x------   1 yarn     hadoop      10621 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/log4j.properties
 25824592      4 -r-x------   1 yarn     hadoop       2123 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hadoop-metrics2.properties
 25824604      4 -r-x------   1 yarn     hadoop       1335 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/configuration.xsl
 25824608      4 -r-x------   1 yarn     hadoop       1020 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/commons-logging.properties
 25824612      4 -r-x------   1 yarn     hadoop       1602 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/health_check
 25824615      4 -r-x------   1 yarn     hadoop       1352 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hadoop-policy.xml
 25824596      4 -r-x------   1 yarn     hadoop          1 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/yarn.exclude
 25824600      4 -r-x------   1 yarn     hadoop          1 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/dfs.exclude
 25824616      4 -r-x------   1 yarn     hadoop         65 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/slaves
 25824599      4 -r-x------   1 yarn     hadoop        244 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/spark-thrift-fairscheduler.xml
 25824607      4 -r-x------   1 yarn     hadoop       1674 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/container-executor.cfg
 25824588      8 -r-x------   1 yarn     hadoop       6195 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hadoop-env.sh
 25824614      4 -r-x------   1 yarn     hadoop       2316 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/ssl-client.xml.example
 25824606     12 -r-x------   1 yarn     hadoop       9223 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hdfs-site.xml
 25824595      4 -r-x------   1 yarn     hadoop        259 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/hadoop-metrics2-azure-file-system.properties
 25824617      4 -r-x------   1 yarn     hadoop        122 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/topology_mappings.data
 25824619      4 -r-x------   1 yarn     hadoop       2697 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/ssl-server.xml.example
 25824611      4 -r-x------   1 yarn     hadoop       1282 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/mapred-env.sh
 25824601      4 -r-x------   1 yarn     hadoop        845 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/ssl-client.xml
 25824610      8 -r-x------   1 yarn     hadoop       4221 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/task-log4j.properties
 25824603      4 -r-x------   1 yarn     hadoop       2358 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/topology_script.py
 25824597     12 -r-x------   1 yarn     hadoop      11445 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/core-site.xml
 25824598      4 -r-x------   1 yarn     hadoop         10 Mar 16 06:37 ./__spark_conf__/__hadoop_conf__/workers
 25824623      8 -r-x------   1 yarn     hadoop       5017 Mar 16 06:37 ./__spark_conf__/__spark_conf__.properties
 25824628      4 drwx------   2 yarn     hadoop       4096 Mar 16 06:37 ./__hive_libs__
 25824629  48644 -r-x------   1 yarn     hadoop   49809546 Mar 16 06:37 ./__hive_libs__/standalone-metastore-1.21.4.4.1.4.8-hive3.jar
 25824625     44 -r-x------   1 yarn     hadoop      42437 Mar 16 06:37 ./py4j-0.10.7-src.zip
 31326329      4 -rw-r--r--   1 yarn     hadoop         16 Mar 16 06:37 ./.default_container_executor.sh.crc
 31326328      4 -rwx------   1 yarn     hadoop        769 Mar 16 06:37 ./default_container_executor.sh
 31326324      8 -rwx------   1 yarn     hadoop       6860 Mar 16 06:37 ./launch_container.sh
 31326323      4 -rw-r--r--   1 yarn     hadoop         12 Mar 16 06:37 ./.container_tokens.crc
 31326327      4 -rw-r--r--   1 yarn     hadoop         16 Mar 16 06:37 ./.default_container_executor_session.sh.crc
broken symlinks(find -L . -maxdepth 5 -type l -ls):

End of LogType:directory.info
*******************************************************************************

Container: container_e145_1647400556153_0012_01_000005 on wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net_30050_1647414834857
LogAggregationType: AGGREGATED
=============================================================================================================================================
LogType:launch_container.sh
LogLastModifiedTime:Wed Mar 16 07:13:54 +0000 2022
LogLength:6860
LogContents:
#!/bin/bash

set -o pipefail -e
export PRELAUNCH_OUT="/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000005/prelaunch.out"
exec >"${PRELAUNCH_OUT}"
export PRELAUNCH_ERR="/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000005/prelaunch.err"
exec 2>"${PRELAUNCH_ERR}"
echo "Setting up env variables"
export JAVA_HOME=${JAVA_HOME:-"/usr/lib/jvm/zulu-8-azure-amd64"}
export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-"/usr/hdp/4.1.4.8/hadoop/conf"}
export HADOOP_YARN_HOME=${HADOOP_YARN_HOME:-"/usr/hdp/4.1.4.8/hadoop-yarn"}
export HADOOP_HOME=${HADOOP_HOME:-"/usr/hdp/4.1.4.8/hadoop"}
export PATH=${PATH:-"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/var/lib/ambari-agent"}
export LANG=${LANG:-"en_US.UTF-8"}
export HADOOP_TOKEN_FILE_LOCATION="/mnt/resource/hadoop/yarn/local/usercache/sshuser/appcache/application_1647400556153_0012/container_e145_1647400556153_0012_01_000005/container_tokens"
export CONTAINER_ID="container_e145_1647400556153_0012_01_000005"
export NM_PORT="30050"
export NM_HOST="wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net"
export NM_HTTP_PORT="30060"
export LOCAL_DIRS="/mnt/resource/hadoop/yarn/local/usercache/sshuser/appcache/application_1647400556153_0012"
export LOCAL_USER_DIRS="/mnt/resource/hadoop/yarn/local/usercache/sshuser/"
export LOG_DIRS="/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000005"
export USER="sshuser"
export LOGNAME="sshuser"
export HOME="/home/"
export PWD="/mnt/resource/hadoop/yarn/local/usercache/sshuser/appcache/application_1647400556153_0012/container_e145_1647400556153_0012_01_000005"
export JVM_PID="$$"
export MALLOC_ARENA_MAX="4"
export NM_AUX_SERVICE_mapreduce_shuffle="AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="
export NM_AUX_SERVICE_spark2_shuffle=""
export SPARK_YARN_STAGING_DIR="wasbs://dhlsg-hd4insight-prod-clus@dhlsghdinsightprodsa.blob.core.windows.net/user/sshuser/.sparkStaging/application_1647400556153_0012"
export PYTHONPATH="/home/sshuser/pysqlite:/home/sshuser/dhl/aqcc/config:/usr/hdp/current/spark2-client/python:/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip:$PWD/pyspark.zip:$PWD/py4j-0.10.7-src.zip"
export SPARK_DIST_CLASSPATH=":/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:"
export CLASSPATH="$PWD:$PWD/__spark_conf__:$PWD/__spark_libs__/*:/usr/hdp/current/spark2-client/jars/*:$HADOOP_CONF_DIR:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*:/usr/hdp/current/hadoop-yarn-client/*:/usr/hdp/current/hadoop-yarn-client/lib/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/4.1.4.8/hadoop/lib/hadoop-lzo-0.6.0.4.1.4.8.jar:/etc/hadoop/conf/secure::/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf::$PWD/__spark_conf__/__hadoop_conf__"
export SPARK_LOG_URL_STDOUT="http://wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net:30060/node/containerlogs/container_e145_1647400556153_0012_01_000005/sshuser/stdout?start=-4096"
export SPARK_USER="sshuser"
export SPARK_LOG_URL_STDERR="http://wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net:30060/node/containerlogs/container_e145_1647400556153_0012_01_000005/sshuser/stderr?start=-4096"
echo "Setting up job resources"
ln -sf "/mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21996/__spark_conf__.zip" "__spark_conf__"
ln -sf "/mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21997/py4j-0.10.7-src.zip" "py4j-0.10.7-src.zip"
ln -sf "/mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21995/pyspark.zip" "pyspark.zip"
ln -sf "/mnt/resource/hadoop/yarn/local/usercache/sshuser/filecache/21998/__hive_libs__6588329932098669616.zip" "__hive_libs__"
echo "Copying debugging information"
# Creating copy of launch script
cp "launch_container.sh" "/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000005/launch_container.sh"
chmod 640 "/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000005/launch_container.sh"
# Determining directory contents
echo "ls -l:" 1>"/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000005/directory.info"
ls -l 1>>"/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000005/directory.info"
echo "find -L . -maxdepth 5 -ls:" 1>>"/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000005/directory.info"
find -L . -maxdepth 5 -ls 1>>"/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000005/directory.info"
echo "broken symlinks(find -L . -maxdepth 5 -type l -ls):" 1>>"/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000005/directory.info"
find -L . -maxdepth 5 -type l -ls 1>>"/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000005/directory.info"
echo "Launching container"
exec /bin/bash -c "LD_LIBRARY_PATH=\"/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64:$LD_LIBRARY_PATH\" $JAVA_HOME/bin/java -server -Xmx49152m '-XX:+UseG1GC' -Djava.io.tmpdir=$PWD/tmp '-Dspark.network.timeout=2000s' '-Dspark.driver.port=45485' '-Dspark.history.ui.port=18080' -Dspark.yarn.app.container.log.dir=/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000005 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@hn0-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net:45485 --executor-id 4 --hostname wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net --cores 5 --app-id application_1647400556153_0012 --user-class-path file:$PWD/__app__.jar 1>/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000005/stdout 2>/mnt/resource/hadoop/yarn/log/application_1647400556153_0012/container_e145_1647400556153_0012_01_000005/stderr"

End of LogType:launch_container.sh
************************************************************************************


End of LogType:prelaunch.err
******************************************************************************

Container: container_e145_1647400556153_0012_01_000005 on wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net_30050_1647414834857
LogAggregationType: AGGREGATED
=============================================================================================================================================
LogType:prelaunch.out
LogLastModifiedTime:Wed Mar 16 07:13:54 +0000 2022
LogLength:100
LogContents:
Setting up env variables
Setting up job resources
Copying debugging information
Launching container

End of LogType:prelaunch.out
******************************************************************************

Container: container_e145_1647400556153_0012_01_000005 on wn101-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net_30050_1647414834857
LogAggregationType: AGGREGATED
=============================================================================================================================================
LogType:stderr
LogLastModifiedTime:Wed Mar 16 07:13:54 +0000 2022
LogLength:6998
LogContents:
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/hdp/4.1.4.8/spark2/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/4.1.4.8/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
22/03/16 07:13:52 ERROR CoarseGrainedExecutorBackend [dispatcher-event-loop-0]: Executor self-exiting due to : Driver hn0-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net:45485 disassociated! Shutting down.
22/03/16 07:13:52 WARN Executor [driver-heartbeater]: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:92)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:841)
	at org.apache.spark.executor.Executor$$anon$2$$anonfun$run$1.apply$mcV$sp(Executor.scala:870)
	at org.apache.spark.executor.Executor$$anon$2$$anonfun$run$1.apply(Executor.scala:870)
	at org.apache.spark.executor.Executor$$anon$2$$anonfun$run$1.apply(Executor.scala:870)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.executor.Executor$$anon$2.run(Executor.scala:870)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to connect to hn0-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net/172.16.1.17:45485
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)
	at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:198)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:194)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:190)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hn0-dhlsg.3pic2xn4u3jefike0mbkj3izme.ix.internal.cloudapp.net/172.16.1.17:45485
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:688)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:635)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:552)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:514)
	at io.netty.util.concurrent.SingleThreadEventExecutor$6.run(SingleThreadEventExecutor.java:1044)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
22/03/16 07:13:54 ERROR CoarseGrainedExecutorBackend [SIGTERM handler]: RECEIVED SIGNAL TERM
22/03/16 07:13:54 WARN JavaUtils [shutdown-hook-0]: Attempt to delete using native Unix OS command failed for path = /mnt/resource/hadoop/yarn/local/usercache/sshuser/appcache/application_1647400556153_0012/blockmgr-97551a11-5c3f-47b5-a021-70a586e24ecd. Falling back to Java IO way
java.io.IOException: Failed to delete: /mnt/resource/hadoop/yarn/local/usercache/sshuser/appcache/application_1647400556153_0012/blockmgr-97551a11-5c3f-47b5-a021-70a586e24ecd
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:171)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:110)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1062)
	at org.apache.spark.storage.DiskBlockManager$$anonfun$org$apache$spark$storage$DiskBlockManager$$doStop$1.apply(DiskBlockManager.scala:178)
	at org.apache.spark.storage.DiskBlockManager$$anonfun$org$apache$spark$storage$DiskBlockManager$$doStop$1.apply(DiskBlockManager.scala:174)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.storage.DiskBlockManager.org$apache$spark$storage$DiskBlockManager$$doStop(DiskBlockManager.scala:174)
	at org.apache.spark.storage.DiskBlockManager$$anonfun$addShutdownHook$1.apply$mcV$sp(DiskBlockManager.scala:156)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

End of LogType:stderr
***********************************************************************


End of LogType:stdout
***********************************************************************

